const Hypercore = require('hypercore')
// const SDK = require('dat-sdk')
const reallyReady = require('hypercore-really-ready')
const ram = require('random-access-memory')
const hyperswarm = require('hyperswarm')
const compress = require('brotli').compress;
const decompress = require('brotli').decompress;
const intercept = require('intercept-hypercore-storage')
const hosterStorage = require('datdot-service/hoster-storage')
const levelup = require('levelup')
const memdown = require('memdown')

const through2 = require('through2')
const duplexify = require('duplexify')

module.exports = datdotservice

function datdotservice (opts) {

  { HosterStorage, profile } = opts

  const API = {
    // brotli w/ level 11 and using Mauve's module
    encode,
    // store encoded hypercore using Mauve's module and make host send extension message on every connection
    host,
    // connects to all peers in swarm until they find hoster (= the one who sends a specific extension message)
    attest,
    // using Mauve's module
    solveChallenge,
    // status messages for all these functions
    on,
  }
  return API

  /* --------------------------------------
                ENCODE
  ----------------------------------------- */
  function encode (request, cb) {
    const { feedkey, swarmkey, eid, hid } = request
    const feed = new Hypercore(ram, feedkey)
    const ehswarmkey = `datdot${chainid}:${eid}:${hid}`
    const efeed = new Hypercore(ram, ehswarmkey)

    feed.ready(() => {
      reallyReady(feed, (err) => {
        if (err) return console.log(err)
        efeed.ready(() => {
          reallyReady(efeed, (err) => {
            if (err) return console.log(err)
            const { hooks, ehooks } = makeHooks()
            intercept(feed, hooks)
            intercept(efeed, ehooks)
            replicate(feed, swarmkey)
            replicate(efeed, ehswarmkey)
            encodeTransfer(feed, efeed)
            efeed.rootHashes(efeed.length, (err, merkleRoot) => {
              cb(merkleRoot)
            })
          })
        })
      })
    })
}
// --------------------------------------------------------------------------
function makeHooks () {
  var wait1
  const array1 = []
  function noCapacity1 () { return false }

  function put1 (index, data, cb) { // receive chunk from publisher
    const slot = array1[index]
    if (slot) {
      slot(null, data)
      slot = undefined
    } else { slot = data }
    if (noCapacity1()) wait1 = cb
    else cb(null)
  }
  function get1 (index, cb) { // serve chunk to efeed
    const slot = array1[index]
    if (slot) {
      cb(null, slot)
      slot = undefined
      if (wait1 && !noCapacity1()) {
        wait1(null)
        wait1 = undefined
      }
    } else { slot = cb }
    cb(null, thedata)
  }

  var wait2
  const array2 = []
  const rootHashes2 = [] // merkle tree
  function noCapacity2 () { return false }
  function put2 (index, data, cb) { // receive chunk from feed
    const slot = array2[index]
    if (slot) {
      slot(null, data)
      slot = undefined
    } else { slot = data }
    if (noCapacity2()) wait2 = cb
    else cb(null)
  }
  function get2 (index, cb) { // serve chunk to hoster
    const slot = array2[index]
    if (slot) {
      cb(null, slot)
      slot = undefined
      if (wait2 && !noCapacity2()) {
        wait2(null)
        wait2 = undefined
      }
    } else { slot = cb }
    cb(null, thedata)
  }
  const hooks = { putData: put1, getData: get1 }
  const ehooks = { putData: put2, getData: get2 }
  return { hooks, ehooks }
}
// --------------------------------------------------------------------------
function encodeTransfer (feed, efeed) {
  const stream1 = feed.createReadStream()
  const stream2 = through2(compression)
  const stream3 = efeed.createWriteStream()
  stream1.pipe(stream2).pipe(stream3)
  function compression (chunk, enc, next) {
    var self = this
    const opts = {
      mode: 0, // 0 = generic, 1 = text, 2 = font (WOFF2)
      quality: 11, // 0 - 11
      lgwin: 22 // window size
    }
    self.push(compress(chunk, opts))
    next()
  }
}
/* --------------------------------------
              HOST
----------------------------------------- */
// @TODO separate encoder and host from Mauve's code

  function host (request, cb) {
    const { feedkey, swarmkey, eid, hid } = request
    ;(async () => {
      // Initialize SDK with in-memory storage
      // const sdk1 = await SDK({
      //   storage: RAM
      // })

      // Initialize second SDK in-memory storage
      // const sdk2 = await SDK({
      //   storage: RAM
      // })

      try {
        // Initialize levelup instance with in-memory storage
        const db = levelup(memdown())

        // Initialize feed in second SDK from key in feed, in sparse mode
        const hostedFeed = sdk2.Hypercore(feedkey, {
          sparse: true
        })

        console.log('Initializing host')
        // Pass to storage
        // const host = new HosterStorage({ encode, decode }, db, hostedFeed)

        // await reallyReady(hostedFeed)

        // JSON encode the data from the feed
        const encodedData = await EncoderDecoder.encode(SAMPLE_DATA)

        console.log('Storing encoded data in host')
        // Try sending encoding to storage
        await host.storeEncoded(0, SAMPLE_PROOF, encodedData)

        console.log('Getting data from host')
        // Get the data from the host's feed
        const storedData = await hostedFeed.get(0)

        const isSame = storedData.equals(SAMPLE_DATA) // unencoded data
        console.log('Host data is same:', isSame)

        const { proof } = await host.getProofOfStorage(0)

        const isSameProof = proof.equals(SAMPLE_PROOF)

        console.log('Host returned expected proof:', isSameProof)
      } catch (e) {
        console.error(e.stack)
      } finally {
        setTimeout(() => {
          sdk1.close()
          sdk2.close()
        }, 1000)
      }
    })()

  }
  function host (request, cb) {
    const { feedkey, swarmkey, eid, hid } = request
    const feed = new Hypercore(ram, feedkey)
    const ehswarmkey = `datdot${chainid}:${eid}:${hid}`
    const efeed = new Hypercore(HosterStorage, ehswarmkey)
    const db = levelup(memdown())
    const EncoderDecoder = { encode, decode }
    const SAMPLE_DATA
    const SAMPLE_PROOF

    feed.ready(() => {
      reallyReady(feed, (err) => {
        if (err) return console.log(err)
        efeed.ready(() => {
          reallyReady(efeed, (err) => {
            const host = new HosterStorage(EncoderDecoder, db, feed)
            replicate(feed, swarmkey)
            replicate(efeed, ehswarmkey)
            // const RANDOM_INDEX = getRandomizer(0, feed.length)
            feed.get(0, (err, res) => { SAMPLE_DATA = res }) // original chunk
            efeed.get(0, (err, res) => { SAMPLE_PROOF = res }) // encoded chunk
            const encodedData = await EncoderDecoder.encode(SAMPLE_DATA) // encoded chunk

            await host.storeEncoded(0, SAMPLE_PROOF, encodedData)

            const storedData
            feed.get(0, (err, res) => { storedData = res })
            const isSame = storedData.equals(SAMPLE_DATA) // unencoded data
            const { proof } = await host.getProofOfStorage(0)

            const isSameProof = proof.equals(SAMPLE_PROOF)

            console.log('Host returned expected proof:', isSameProof)
            // if (err) return console.log(err)
            // hosterStorage({ encode, decode}, db, efeed)
            // efeed.rootHashes(efeed.length, (err, merkleRoot) => {
            //   cb(merkleRoot)
            // })

          })
        })
      })
    })
  }

  // // ---------------------------------------------------------------------
  async function encode(rawData) () {
    const opts = {
      mode: 0, // 0 = generic, 1 = text, 2 = font (WOFF2)
      quality: 11, // 0 - 11
      lgwin: 22 // window size
    }
    compress(rawData, opts)
  }
  async function decode(encodedData) () {
    decompress(encodedData)
  }

  function getRandomizer(bottom, top) {
    return Math.floor( Math.random() * ( 1 + top - bottom ) ) + bottom;
  }
  /* --------------------------------------
                ATTEST
  ----------------------------------------- */
  function attest (challenge, cb) {
    const { chunkIndexes} = challenge
    const proof = []
    chunkIndexes.forEach(index => {
      proof.push(hosterStorage.getProofOfStorage(index))
    })
    cb(proof)
  }

  /* --------------------------------------
                SOLVE CHALLENGE
  ----------------------------------------- */
  function solveChallenge (challenge, cb) {
    const { hosterID, planID, chunkIndexes} = challenge
  }
  /* --------------------------------------
                LISTEN TO EVENTS
  ----------------------------------------- */
  const listen = { on, off, once }
  function on (query, callback) {

  }
  function off (query, callback) {

  }
  function once (query, callback) {

  }
  // example usage
  // const handler = (err, data) => { }
  // listen.on(query, handler)
  // listen.off(query, handler)
  // listen.once(query, handler)

  /* --------------------------------------
                HELPERS
  ----------------------------------------- */

  function replicate (_feed, _swarmkey) {
    const swarm = hyperswarm()
    swarm.join(_swarmkey, { lookup: true, announce: true })
    swarm.on('connection', function (socket, info) {
      socket.pipe(_feed.replicate(info.client)).pipe(socket)
    })
  }

  // function encode (opts, proof, done) {
  //   if (!profile.verify(opts, proof)) throw new Error('verify signature!')
  //
  //   const { address } = opts
  //   const swarmE = hyperswarm()
  //   const feedE = hypercore() // @NOTE: use memory
  //   // @TODO: optimize, by not storing on disk, but forwarding directly
  //   const swarm = hyperswarm()
  //   const feed = hypercore(address) // @NOTE: use memory
  //   feed.on('ready', () => feedE.on('ready', () => {
  //     swarmE.join(hosterID, { announce: true })
  //     swarmE.on('connection', s => s.pipe(feedE.replicate(false)).pipe(s))
  //     swarm.join(feed.key, { lookup: true })
  //     swarm.on('connection', s => s.pipe(feed.replicate(true)).pipe(s))
  //
  //     // LESEZEICHEN:
  //     // @TODO: done('start syncing with hoster')
  //
  //
  //     feed.createReadStream().on('data', chunk => {
  //       compress(chunk, (err, chunkE) => feedE.append(chunkE))
  //     }).on('end', () => {
  //       // @TODO: notify the chain (done(encodedMerkleRoot))
  //       var merkleRootE = feedE.getMerkleRoot()
  //       done()
  //
  //       // LESEZEICHEN:
  //       // @TODO: done('commit to chain')
  //       // @TODO: done('listen for final done and end everything - cleanup')
  //
  //
  //       chainAPI.commit(myAccountId, event, merkleRootE)
  //     })
  //
  //   }))
  // }

  //
  // Helpers
  //
  // getData // from swarm
  // getChunk

}
